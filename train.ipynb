{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "import torch\n",
    "\n",
    "from model import ConvStackModel\n",
    "from dataset import MNISTDataset\n",
    "\n",
    "# sci_mode=True is the default which will print values such as 9.8e-01 instead of 0.98.\n",
    "torch.set_printoptions(precision=3, sci_mode=False)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup the dataloader, model, loss function & optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataset_train = MNISTDataset(split=\"train\")\n",
    "batch_size = 32\n",
    "mnist_data_loader_train = torch.utils.data.DataLoader(dataset=mnist_dataset_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = ConvStackModel()\n",
    "\n",
    "bce_loss_fn = torch.nn.BCELoss(reduction='sum')\n",
    "adam_optimizer = torch.optim.Adam(params=model.parameters(), lr=4e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pull an example element & example batch from the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A DataLoader is a Python Iterable (not to be confused with an Iterator). As the name suggests, an Iterable\n",
    "# is able to be iterated over. The iteration is supported or done by a seperate object called an Iterator. Generally,\n",
    "# an Iterable implements the __iter__ method which creates & returns an Iterator object.\n",
    "mnist_data_loader_train_iterator = iter(mnist_data_loader_train)\n",
    "\n",
    "batch_of_images, batch_of_labels = next(mnist_data_loader_train_iterator)\n",
    "single_image, single_label = mnist_dataset_train[672]\n",
    "# Add a batch-dimension with a single entry i.e. of size 1.\n",
    "single_image = single_image.unsqueeze(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training-step using one image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model(single_image)\n",
    "probability_of_correct_class = predictions[:, single_label.item()].squeeze()\n",
    "loss_score = bce_loss_fn(input=probability_of_correct_class, target=torch.tensor(1.0))\n",
    "\n",
    "print(f\"Model predicted a probability of {probability_of_correct_class:.2f} for the correct clas.\")\n",
    "print(f\"Loss score: {loss_score:.3f}.\")\n",
    "\n",
    "loss_score.backward()\n",
    "adam_optimizer.step()\n",
    "adam_optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training-step using a batch of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model(batch_of_images)\n",
    "\n",
    "correct_class_indices = batch_of_labels.unsqueeze(dim=1).to(torch.int64)\n",
    "probability_of_correct_classes = predictions.gather(dim=1, index=correct_class_indices).squeeze()\n",
    "\n",
    "loss_score = bce_loss_fn(input=probability_of_correct_classes, target=torch.ones(size=(batch_size,)))\n",
    "\n",
    "# The inferred joint-probability predicted for every correct class in the batch. Change the data-type from float32 to float64\n",
    "# to reduce the likelihood of underflowing to 0.\n",
    "joint_probability = probability_of_correct_classes.to(torch.float64).prod()\n",
    "# We can also recover this joint-probability from the loss-score, since we know the loss is computed as -log(joint_probability).\n",
    "joint_probability_from_loss = math.exp(-loss_score)\n",
    "# The joint-probability is the product of all probabilities, so we invert that step to get the per batch-element \n",
    "# average probability.\n",
    "per_batch_element_average_probability = joint_probability_from_loss ** (1/batch_size)\n",
    "\n",
    "print(f\"Joint probability: {joint_probability:.2e}. Joint probability inferred from loss-score: {joint_probability_from_loss:.2e}.\")\n",
    "print(f\"The per batch-element average probability is: {per_batch_element_average_probability:.3f}.\")\n",
    "print(f\"Loss score: {loss_score:.3f}\")\n",
    "\n",
    "loss_score.backward()\n",
    "adam_optimizer.step()\n",
    "adam_optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train for num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "num_batches_per_epoch = math.ceil(len(mnist_dataset_train) / batch_size)\n",
    "num_batches_to_print_per_epoch = 10\n",
    "\n",
    "for epoch_idx in range(num_epochs):\n",
    "    print(f\"\\nBeginning epoch: {epoch_idx + 1}.\")\n",
    "    \n",
    "    for batch_idx, (batch_of_images, batch_of_labels) in enumerate(mnist_data_loader_train):\n",
    "        \n",
    "        predictions = model(batch_of_images)\n",
    "        \n",
    "        correct_class_indices = batch_of_labels.unsqueeze(dim=1).to(torch.int64)\n",
    "        probability_of_correct_classes = predictions.gather(dim=1, index=correct_class_indices).squeeze()\n",
    "        \n",
    "        loss_score = bce_loss_fn(input=probability_of_correct_classes, target=torch.ones_like(probability_of_correct_classes))\n",
    "        joint_probability_from_loss = math.exp(-loss_score)\n",
    "\n",
    "        # Note: the last batch may contain less than batch-size elements.\n",
    "        per_batch_element_average_probability = joint_probability_from_loss ** (1 / len(batch_of_labels))\n",
    "\n",
    "        # Only print the first few batches of results for each epoch.\n",
    "        if batch_idx < num_batches_to_print_per_epoch:\n",
    "            print(\n",
    "                f\"Batch: {batch_idx + 1}. \"\n",
    "                f\"Joint probability inferred from loss-score: {joint_probability_from_loss:.2e}. \"\n",
    "                f\"Per batch-element average probability is: {per_batch_element_average_probability:.3f} \"\n",
    "                f\"Loss score: {loss_score:.3f}\"\n",
    "            )\n",
    "            \n",
    "            # Add a sleep to be able to watch in real-time. Otherwise, the process is too quick.\n",
    "            time.sleep(2)\n",
    "        \n",
    "        elif batch_idx == num_batches_to_print_per_epoch:\n",
    "            print(\n",
    "                f\"Accelerating through the remaining {num_batches_per_epoch - num_batches_to_print_per_epoch:,} \"\n",
    "                f\"batches in this epoch without printing updates...\"\n",
    "            )\n",
    "        \n",
    "\n",
    "        loss_score.backward()\n",
    "        adam_optimizer.step()\n",
    "        adam_optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.3-mnist",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
